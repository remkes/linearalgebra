<section xml:id="inversion">
  <title>Inverse Linear Transforms and Matrix Inversion</title>
  <subsection xml:id="inverse-transforms">
    <title>Inverse Transforms</title>
    <p>
      When we defined Dihedral Groups, we already talked briefly about
      inverse linear transformation. Let's give a general
      definition.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>T: \RR^n \rightarrow \RR^n</m>. The inverse
          transformation of <m>T</m> is a transformation <m>S: \RR^n
          \rightarrow \RR^n</m> which undoes what <m>T</m> did.
          Equivalent, it is a transformation <m>S</m> such that the
          compositions <m>T \circ S</m> and <m>S \circ T</m> are the
          identity. For an arbitrary transformation, there is no
          guarantee that an inverse exists; if it does, it is written
          <m>T^{-1}</m>.
        </p>
      </statement>
    </definition>
    <p>
      This is the geometric definition. Now that we've encoded
      transformations as matrices, we can use matrices and ask for an
      algebriac definition of the inverse as well.
    </p>
  </subsection>
  <subsection xml:id="inverse-matrices">
    <title>Inverse Matrices</title>
    <p>
      The identity property: <m>AI_l = I_k A = A</m> reminds us of
      multiplication by one in <m>\RR</m>. This is the multiplication
      that doesn't accomplish anything. In number systems, when we
      have an identity, we usually have a way of getting back to that
      identity. Zero is the identity in addition. For any number
      <m>a</m>, we have <m>(-a)</m> so that <m>a + (-a) = 0</m> gets
      us back to the identity. One is the identity in multiplication.
      For any <em>non-zero</em> number <m>a</m>, we have
      <m>\frac{1}{a}</m> so that <m>a \frac{1}{a} = 1</m> gets us back
      to the identity.
    </p>
    <p>
      For matrices, we have the same question. For any matrix
      <m>M</m>, is there another matrix <m>N</m> such that <m>MN =
      I</m>?  Multiplication of numbers alreay shows us that we need
      to take care: for the number zero, there is no such number. For
      matrices, we have to be even more cautious.
    </p>
    <definition>
      <statement>
        <p>
          Let <m>M</m> be a <m>n \times n</m> (square) matrix. The
          the <term>inverse</term> of <m>M</m> is the
          <term>unique</term> matrix <m>M^{-1}</m> (if it exists) such
          that <m>MM^{-1} = M^{-1}M = I_n</m>.
        </p>
      </statement>
    </definition>
    <p>
      We should note a couple of thing about this definition. First,
      it only applies to square matrices. We don't even try to invert
      non-square matrices. Second, we need to have both orders of
      multiplication <m>MM^{-1}</m> and <m>M^{-1}M</m>. This is due
      to the previous observation that matrix multiplication is
      non-commutative. In general, these two orders could result in
      different products. In this case, we insist that both orders
      get us back to the identity.
    </p>
  </subsection>
  <subsection xml:id="calculation-inverse-matrices">
    <title>Calculating Inverse Matrices</title>
    <p>
      The definition is good, but we are left with the problem of
      determining which matrices have inverses and calculating those
      inverses. It turns out there is a convenient algorithm using
      techniques we already know. We write an extended matrix
      <m>(M|I_n)</m> with our original matrix and the identity next to
      each other, separated by a vertical line. Then we row reduce,
      treating the conglomeration as one matrix with long rows. If
      our row reduction of <m>M</m> results in the identity on the
      left, then the matrix has the form <m>(I|M^{-1})</m>; the right
      hand side will be the inverse.
    </p>
    <p>
      This algorithm gives us two observations. First, obviously, it
      is a way to calculate inverses. But it also is a condition: an
      <m>n \times n</m> matrix is invertible, in this algorithm, only
      if we get the identity matrix on the left after row reduction.
      This is part of a general result, which is the first of several
      conditions for intertible matrices.
    </p>
    <proposition>
      <statement>
        <p>
          A <m>n \times n</m> matrix is invertible if and only if it
          row reduces to the identity matrix <m>I_n</m>. Since the
          identity has <m>n</m> leading ones, this is equivalent to
          the matrix having rank <m>n</m>.
        </p>
      </statement>
    </proposition>
    <definition>
      <statement>
        <p>
          The set of all intervible <m>n \times n</m> matrices with
          real coefficient forms a group. It is called the
          <term>General Linear Group</term> and written
          <m>GL_n(\RR)</m>. If we wanted to change the coefficients
          to another set of scalars <m>S</m>, we would write
          <m>GL_n(S)</m>. When the coefficients are understood, we
          sometime write <m>GL_n</m> or <m>GL(n)</m>.
        </p>
      </statement>
    </definition>
  </subsection>
</section>
