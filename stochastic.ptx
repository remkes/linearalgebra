<section xml:id="stochastic">
  <title>Stochastic Matrices and Markov Chains</title>
  <p>
    Markov chains are probabalistic linear dynamical systems. Like
    all dynamical systems, we have a state vector <m>v \in \RR^n</m>.
    Here, <m>v</m> represents a probablistic state and the iterated
    process asks: what are the probabilities in the next state?
    Often, Markov chains are given graph theoretic interpretation. If
    <m>v \in \RR^n</m>, then we can consider a graph on <m>n</m>
    vertices. Each coefficient of the matrix <m>A_{ij}</m> is the
    probability of going from vertex <m>j</m> to vertex <m>i</m>. In
    terms of the graph, these transition number can be thought of as a
    label on a directed edge.
  </p>
  <p>
    In this interpretation, the <m>i</m>th column gives the total
    probabilities of all departures from the <m>i</m>th vertex
    (including <m>A_{ii}</m>, the probability of staying put). Since
    something must happen in each time step, we insist that the column
    sums, <m>r_j = \sum_i a_{ij}</m>, all must equal one.
  </p>
  <definition>
    <statement>
      <p>
        A non-negative <m>n \times n</m> matrix where all the columns
        sum to one is called a <term>(left) stochastic matrix</term>.
        (There is a similar definition of a righ stochastic matrix
        where all the rows sum to one; it is, in fact, the more common
        definition. However, it doesn't mesh that well with our
        conventions of columns vectors and matrix actions, so we will
        work with left stochastic matrices and surpress the word
        <sq>left</sq> in what follows.)
      </p>
    </statement>
  </definition>
  <p>
    Stochastic matrices reflect a probabalistic interpretation. They
    are are non-negative, so the weak form of Perron-Frobenius
    applies, but they may not be irreducible. (In this graph, this
    depends on whether or not each vertex is reachible from all the
    other vertices).
  </p>
  <p>
    As with previous iterated processes, we are curious about the long
    term behaviour. However, for stochastic matrices are are less
    directly concerned with eigenvalues and eigenvectors. (We can
    prove, easily from the column sums, that the dominant eigenvalue
    is <m>\lambda = 1</m> and the dominant eigenvector is the vector
    where all coefficients sum to 1.) Instead, we are more interested
    in the matrix <m>A^n</m> itself. The higher powers of the matrix
    calculate the iterated process. Therefore, we can interpret the
    coefficient <m>(A^k)_{ij}</m> as the probability of starting in
    vextex <m>j</m> and ending up in vertex <m>i</m> after <m>k</m>
    timesteps.
  </p>
</section>
