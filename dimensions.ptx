<section xml:id="dimensions">
  <title>Dimensions of Spans and Loci</title>
  <subsection xml:id="rank">
    <title>Rank</title>
    <definition>
      <statement>
        <p>
          Let <m>A</m> be a <m>m \times n</m> matrix. The
          <term>rank</term> of <m>A</m> is the number of leading ones
          in its reduced row-echelon form. 
        </p>
      </statement>
    </definition>
  </subsection>
  <subsection xml:id="dimensions-of-spans">
    <title>Dimensions of Spans</title>
    <p>
      We can now define the desired techniques to solve the dimension
      problems that we presented earlier. For loci and spans, we
      didn't have a way of determining what information was redundant.
      Row-reduction of matrices gives us the tool we need.
    </p>
    <p>
      Given some vectors <m>\{v_1, v_2, \ldots, v_k\}</m>, what is the
      dimension of <m>\Span \{ v_1, v_2, \ldots, v_k\}</m>?  By
      definition, it is the number of linearly independent vectors in
      the set. Now we can test for linearly independent vectors.
    </p>
    <proposition>
      <statement>
        <p>
          Let <m>\{v_1, v_2, \ldots, v_k\}</m> be a set of vectors in
          <m>\RR^n</m>. If we make these vectors the rows of a matrix
          <m>A</m>, then the rank of the matrix <m>A</m> is the number
          of linearly independent vectors in the set. Moreover, if we
          do the row reduction without exchanging rows (which is
          always possible), then the vectors which correspond to rows
          with leading ones in the reduced row-echelon form form a
          maximal linearly independent set, i.e., a basis. Any vector
          corresponding to a row without a leading one is a redundant
          vector in the span. The set is linearly independent if and
          only if the rank of <m>A</m> is <m>k</m>.
        </p>
      </statement>
    </proposition>
  </subsection>
  <subsection xml:id="bases-for-spans">
    <title>Bases for Spans</title>
    <p>
      If <m>\Span\{v_1, v_2, v_3\}</m> has dimension two and the first
      two vectors form a basis, then <m>v_3</m> is a redundant part of
      the span. This means that we can write <m>v_3</m> as a linear
      combination of the first two vectors. That is, there are two
      constants, <m>a</m> and <m>b</m>, such that <m>v_3 = av_1 +
      bv_2</m>. We would like to be able to determine these
      constants. Matrices and row-reduction is again the tool we turn
      turn.
    </p>
    <example>
      <statement>
        <p>
          Let's work by example and consider <m>\Span \{
          \begin{pmatrix} -2 \\ 1 \\ -1 \end{pmatrix}, \begin{pmatrix}
          3 \\ -2 \\ 3\end{pmatrix}, \begin{pmatrix} -4 \\ -4 \\
          7\end{pmatrix} \}</m>. If we make these rows of a matrix
          and row-reduce, we find only the first two rows have leading
          one; therefore, the first vector is redundant in the spane.
          Therefore, we should be able to find number <m>a</m> and
          <m>b</m> in the following equation.
          <me>
            \begin{pmatrix} -4 \\ -4 \\ 7 \end{pmatrix} = a
            \begin{pmatrix} -2 \\ 1 \\ -1 \end{pmatrix} + b
            \begin{pmatrix} 0 \\ -2 \\ 3 \end{pmatrix}
          </me>
        </p>
        <p>
          Let's write each components of this vector equation
          seperately.
          <md>
            <mrow>
              -4 \amp  = -2a + 0b
            </mrow>
            <mrow>
              -4 \amp  = a + (-2)b
            </mrow>
            <mrow>
              7 \amp  = -1a + 3b
            </mrow>
          </md>
        </p>
        <p>
          This is a just a new linear system, with three equations and
          two variables, <m>a</m> and <m>b</m>. We can solve it as we
          did before, either directly or using a matrix and row
          reduction. In this case, the linear system is solved by
          <m>a = 2</m> and <m>b=3</m>.
          <me>
            \begin{pmatrix} -4 \\ -4 \\ 7 \end{pmatrix} = 2
            \begin{pmatrix} -2 \\ 1 \\ -1 \end{pmatrix} + 3
            \begin{pmatrix} 0 \\ -2 \\ 3 \end{pmatrix}
          </me>
        </p>
      </statement>
    </example>
    <p>
      This example can be generalized. Any time some vector <m>u \in
      \dollarRR^n</m> is in <m>\Span\{v_1, v_2, \ldots, v_k\}</m>,
      there are constants <m>a_i</m> such that <m>u = a_1v_1 + a_2v_2
      + \ldots + a_kv_k</m>. Writing each component of the vector
      equation seperately gives a system of <m>n</m> different linear
      equation, which we solve as before.
    </p>
    <p>
      This gives a general algorithm for expressing vector in terms of
      a new basis. As we discussed before, there are many different
      bases for linear spaces. The standard basis for <m>\RR^3</m> is
      are the axis vectors <m>e_1</m>, <m>e_2</m> and <m>e_3</m>.
      However, sometimes we might want to use a different basis. Any
      three linearly independent vectors in <m>\RR^3</m> form a
      basis.
    </p>
    <example>
      <statement>
        <p>
          Take the basis <m>\left\{ \begin{pmatrix} 4 \\ -1 \\ -1
          \end{pmatrix} \begin{pmatrix} 2 \\ 0 \\ -3 \end{pmatrix}
          \begin{pmatrix} 0 \\ 2 \\ 7 \end{pmatrix} \right}.</m>.
          Let's express the vector <m>\begin{pmatrix} 1 \\ 1 \\ 2
          \end{pmatrix}</m> in terms of this basis. We are looking
          for constants <m>a</m>, <m>b</m> and <m>c</m> that solve
          this vector equation.
          <me>
            \begin{pmatrix} 1 \\ 1 \\ 2 \end{pmatrix} = a
            \begin{pmatrix} 4 \\ -1 \\ -1 \end{pmatrix} + b
            \begin{pmatrix} -2 \\ 0 \\ -3 \end{pmatrix} + c
            \begin{pmatrix} 0 \\ 2 \\ 7 \end{pmatrix}
          </me>
        </p>
        <p>
          Writen in components, this gives the following linear system.
          <md>
            <mrow>
              3a - 2b + 0c \amp  = 1
            </mrow>
            <mrow>
              -a + 0b + 2c \amp  = 1
            </mrow>
            <mrow>
              -a + 3b + 7c \amp  = 2
            </mrow>
          </md>
        </p>
        <p>
          We translate this into a matrix.
          <me>
            \left( \begin{array}{ccc|c} 3 \amp -2 \amp 0 \amp 1 \\
            -1 \amp 0 \amp 2 \amp 1 \\ -1 \amp 3 \amp 7 \amp 2
            \end{array} \right)
          </me>
        </p>
        <p>
          We row reduce this matrix to get the reduced row-echelon
          form.
          <me>
            \left( \begin{array}{ccc|c} 1 \amp 0 \amp 0 \amp 0 \\ 0
            \amp 1 \amp 0 \amp \frac{1}{2} \\ 0 \amp 0 \amp 1
            \amp \frac{1}{2} \end{array} \right)
          </me>
        </p>
        <p>
          We read that <m>a=0</m>, <m>b=\frac{1}{2}</m> and <m>c =
          \frac{1}{2}</m>.
          <me>
            \begin{pmatrix} 1 \\ 1 \\ 2\end{pmatrix} = 0
            \begin{pmatrix} 3 \\ -1 \\ -1 \end{pmatrix} + \frac{1}{2}
            \begin{pmatrix} -2 \\ 0 \\ -3 \end{pmatrix} + \frac{1}{2}
            \begin{pmatrix} 0 \\ 2 \\ 7 \end{pmatrix} = \frac{1}{2}
            \begin{pmatrix} -2 \\ 0 \\ -3 \end{pmatrix} + \frac{1}{2}
            \begin{pmatrix} 0 \\ 2 \\ 7 \end{pmatrix}
          </me>
        </p>
      </statement>
    </example>
  </subsection>
  <subsection xml:id="dimensions-of-loci">
    <title>Dimensions of Loci</title>
    <p>
      Similarly, we can use row-reduction of matrices to find the
      dimensions of a locus. In particular, we will be able to
      determine if any of the equations were redundant. Recall that
      we described any affine or linear subspace of <m>\RR^n</m> as
      the locus of some finite number of linear equations: the set of
      points that satisfy a list of equations.
      <md>
        <mrow>
          a_{11} x_1 + a_{12} x_2 + \ldots a_{1n} x_n \amp  = c_1
        </mrow>
        <mrow>
          a_{21} x_1 + a_{22} x_2 + \ldots a_{2n} x_n \amp  = c_2
        </mrow>
        <mrow>
          \ldots \amp
        </mrow>
        <mrow>
          a_{m1} x_1 + a_{m2} x_2 + \ldots a_{mn} x_n \amp  = c_m
        </mrow>
      </md>
    </p>
    <p>
      This is just a system of linear equations, which has a
      corresponding matrix.
      <me>
        \left( \begin{array}{cccc|c} a_{11} \amp a_{12} \amp \ldots
        \amp a_{1n} \amp c_1 \\ a_{21} \amp a_{22} \amp \ldots
        \amp a_{2n} \amp c_2 \\ \ldots \amp \ldots \amp \ldots
        \amp \ldots \amp \ldots \\ a_{11} \amp a_{12} \amp \ldots
        \amp a_{1n} \amp c_1 \end{array} \right)
      </me>
    </p>
    <p>
      So, by definition, the locus of a set of linear equation is just
      the geometric version of the solution space of the system of
      linear equations. Loci and solutions spaces are exactly the
      same thing; only loci are geometry and solution spaces are
      algebra. The dimension of a locus is same dimensions of a
      solution space of a system. Fortunately, we already know how to
      figure that out.
    </p>
    <proposition>
      <statement>
        <p>
          Consider a locus defined by a set of linear equations. Then
          the dimension of the locus is the number of free variables
          in the solution space of the matrix corresponding to the
          system of the equations. If the matrix contains a row that
          leads to a contradction <m>0=1</m>, then the locus is
          empty.
        </p>
      </statement>
    </proposition>
    <p>
      To understand a locus, we just solve the associated system. If
      it has solutions, we count the free variables: that's the
      dimenion of the locus. We can get even more information from
      this process  When we row reduce the matrix <m>A</m>
      corresponding to the linear system, the equations corresponding
      to rows with leading ones (keeping track of exchanging rows, if
      necessary) are equations which are necessary to define the
      locus. Those which end up a rows without leading ones are
      redundant and the locus is unchanged if those equations are
      removed.
    </p>
    <p>
      If the ambient space is <m>\RR^n</m>, then our equations have
      <m>n</m> variables. That is, there are <m>n</m> columns to the
      right of the vertical line in the extended matrix. If the rank
      of <m>A</m> is <m>k</m>, and there are no rows that reduce to
      <m>0=1</m>, then there will be <m>n-k</m> columns without
      leading ones, so <m>n-k</m> free variables. The dimension of
      the locus is the ambient dimension <m>n</m> minus the rank of
      the matrix <m>k</m>.
    </p>
    <p>
      This fits our intuition for spans and loci. Spans are built up:
      their dimension is equal to the rank of an associated matrix,
      since each leading one corresponds to a unique direction in the
      span, adding to the dimension by one. Loci are restictions down
      from the total space: their dimension is the ambient dimension
      minus the rank of an associated matrix, since each leading one
      corresponds to a real restriction on the locus, dropping the
      dimensions by one. In either case, the question of calculating
      dimension boils down to the rank of an associated matrix.
    </p>
  </subsection>
  <subsection xml:id="bases-for-loci">
    <title>Bases for Loci</title>
    <p>
      If we have a solution space for a linear system expressed in
      free variables, we can write it as a vector sum using those free
      variables.
    </p>
    <example>
      <statement>
        <p>
          The following is an example of a solution space in
          <m>\RR^4</m>, with variables <m>w</m>, <m>x</m>, <m>y</m>,
          and <m>z</m>, where <m>y</m> and <m>z</m> are free
          variables.
          <me>
            \begin{pmatrix} w \\ x \\ y \\ z \end{pmatrix} =
            \begin{pmatrix} 1 \\ -4 \\ 0 \\ 0 \end{pmatrix} + y
            \begin{pmatrix} 1 \\ -2 \\ 1 \\ 0 \end{pmatrix} + z
            \begin{pmatrix} -2 \\ -3 \\ 0 \\ 1 \end{pmatrix}
          </me>
        </p>
        <p>
          This is an offset span. The span is all combinations of the
          vectors <m> \begin{pmatrix} 1 \\ -2 \\ 1 \\ 0 \end{pmatrix}
          </m> and <m> \begin{pmatrix} -2 \\ -3 \\ 0 \\ 1\end{pmatrix}
          </m> . Therefore, those two vectors are the basis for the
          span. An offset space itself doesn't have a basis, since
          basis is only defined for linear subspaces.
        </p>
        <p>
          This gives us a way to describe the basis for any locus:
          write it as a system, find the solution space by free
          parameter and idenfity the basis of the linear part.
        </p>
      </statement>
    </example>
  </subsection>
</section>
